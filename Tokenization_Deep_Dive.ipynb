{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "84ec3ee5",
      "metadata": {
        "id": "84ec3ee5"
      },
      "source": [
        "# Tokenization Deep Dive: How LLMs Slice Text\n",
        "**Before an LLM tastes your words, it needs to slice them.**\n",
        "\n",
        "In our companion post, we likened tokenization to slicing a sandwich. Now, letâ€™s go deeper.\n",
        "\n",
        "From whitespace to byte-level slicing and explore how different models tokenize inputs, decode them back, and how this all ties into cost and performance.\n",
        "\n",
        "_Letâ€™s begin at the cafÃ© counter..._"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64098fd6",
      "metadata": {
        "id": "64098fd6"
      },
      "source": [
        "## Analogy Recap: Your Sentence = A Sandwich\n",
        "Each token is a slice. Different slicing tools lead to different results:\n",
        "- **Whitespace Tokenization** â†’ Clean visible cuts\n",
        "- **Subword Tokenization** â†’ Balanced bites like 'Sand' + '##wich'\n",
        "- **Character Tokenization** â†’ Micro-crumbs\n",
        "- **Advanced Models** use: BPE, WordPiece, SentencePiece\n",
        "\n",
        "> More slices = more tokens = more cost."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "266cad1d",
      "metadata": {
        "id": "266cad1d"
      },
      "source": [
        "## Let's Start with a Few Sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "115bfa7a",
      "metadata": {
        "id": "115bfa7a"
      },
      "outputs": [],
      "source": [
        "sentences = [\n",
        "    \"I love spicy sandwiches.\",\n",
        "    \"Tokenization affects both performance and cost.\",\n",
        "    \"Sheldon likes his sandwiches in isosceles triangles.\",\n",
        "    \"ğŸ˜‚ emojis and multilingual à¤¨à¤®à¤¸à¥à¤¤à¥‡ text test subword chops.\",\n",
        "    \"Hashtags like #AI and longwords like Donaudampfschifffahrtsgesellschaft are tricky!\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e498d61",
      "metadata": {
        "id": "6e498d61"
      },
      "source": [
        "## Custom Tokenizers: Whitespace, Regex, Character"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "e713caea",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e713caea",
        "outputId": "2ebda799-cf09-46a8-f8cc-152ef229f034"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ“ Sentence: I love spicy sandwiches.\n",
            "â€¢ Whitespace: ['I', 'love', 'spicy', 'sandwiches.']\n",
            "â€¢ Regex     : ['I', 'love', 'spicy', 'sandwiches', '.']\n",
            "â€¢ Characters: ['I', ' ', 'l', 'o', 'v', 'e', ' ', 's', 'p', 'i', 'c', 'y', ' ', 's', 'a', 'n', 'd', 'w', 'i', 'c', 'h', 'e', 's', '.']\n",
            "\n",
            "ğŸ“ Sentence: Tokenization affects both performance and cost.\n",
            "â€¢ Whitespace: ['Tokenization', 'affects', 'both', 'performance', 'and', 'cost.']\n",
            "â€¢ Regex     : ['Tokenization', 'affects', 'both', 'performance', 'and', 'cost', '.']\n",
            "â€¢ Characters: ['T', 'o', 'k', 'e', 'n', 'i', 'z', 'a', 't', 'i', 'o', 'n', ' ', 'a', 'f', 'f', 'e', 'c', 't', 's', ' ', 'b', 'o', 't', 'h', ' ', 'p', 'e', 'r', 'f', 'o', 'r', 'm', 'a', 'n', 'c', 'e', ' ', 'a', 'n', 'd', ' ', 'c', 'o', 's', 't', '.']\n",
            "\n",
            "ğŸ“ Sentence: Sheldon likes his sandwiches in isosceles triangles.\n",
            "â€¢ Whitespace: ['Sheldon', 'likes', 'his', 'sandwiches', 'in', 'isosceles', 'triangles.']\n",
            "â€¢ Regex     : ['Sheldon', 'likes', 'his', 'sandwiches', 'in', 'isosceles', 'triangles', '.']\n",
            "â€¢ Characters: ['S', 'h', 'e', 'l', 'd', 'o', 'n', ' ', 'l', 'i', 'k', 'e', 's', ' ', 'h', 'i', 's', ' ', 's', 'a', 'n', 'd', 'w', 'i', 'c', 'h', 'e', 's', ' ', 'i', 'n', ' ', 'i', 's', 'o', 's', 'c', 'e', 'l', 'e', 's', ' ', 't', 'r', 'i', 'a', 'n', 'g', 'l', 'e', 's', '.']\n",
            "\n",
            "ğŸ“ Sentence: ğŸ˜‚ emojis and multilingual à¤¨à¤®à¤¸à¥à¤¤à¥‡ text test subword chops.\n",
            "â€¢ Whitespace: ['ğŸ˜‚', 'emojis', 'and', 'multilingual', 'à¤¨à¤®à¤¸à¥à¤¤à¥‡', 'text', 'test', 'subword', 'chops.']\n",
            "â€¢ Regex     : ['ğŸ˜‚', 'emojis', 'and', 'multilingual', 'à¤¨à¤®à¤¸', 'à¥', 'à¤¤', 'à¥‡', 'text', 'test', 'subword', 'chops', '.']\n",
            "â€¢ Characters: ['ğŸ˜‚', ' ', 'e', 'm', 'o', 'j', 'i', 's', ' ', 'a', 'n', 'd', ' ', 'm', 'u', 'l', 't', 'i', 'l', 'i', 'n', 'g', 'u', 'a', 'l', ' ', 'à¤¨', 'à¤®', 'à¤¸', 'à¥', 'à¤¤', 'à¥‡', ' ', 't', 'e', 'x', 't', ' ', 't', 'e', 's', 't', ' ', 's', 'u', 'b', 'w', 'o', 'r', 'd', ' ', 'c', 'h', 'o', 'p', 's', '.']\n",
            "\n",
            "ğŸ“ Sentence: Hashtags like #AI and longwords like Donaudampfschifffahrtsgesellschaft are tricky!\n",
            "â€¢ Whitespace: ['Hashtags', 'like', '#AI', 'and', 'longwords', 'like', 'Donaudampfschifffahrtsgesellschaft', 'are', 'tricky!']\n",
            "â€¢ Regex     : ['Hashtags', 'like', '#', 'AI', 'and', 'longwords', 'like', 'Donaudampfschifffahrtsgesellschaft', 'are', 'tricky', '!']\n",
            "â€¢ Characters: ['H', 'a', 's', 'h', 't', 'a', 'g', 's', ' ', 'l', 'i', 'k', 'e', ' ', '#', 'A', 'I', ' ', 'a', 'n', 'd', ' ', 'l', 'o', 'n', 'g', 'w', 'o', 'r', 'd', 's', ' ', 'l', 'i', 'k', 'e', ' ', 'D', 'o', 'n', 'a', 'u', 'd', 'a', 'm', 'p', 'f', 's', 'c', 'h', 'i', 'f', 'f', 'f', 'a', 'h', 'r', 't', 's', 'g', 'e', 's', 'e', 'l', 'l', 's', 'c', 'h', 'a', 'f', 't', ' ', 'a', 'r', 'e', ' ', 't', 'r', 'i', 'c', 'k', 'y', '!']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def whitespace_tokens(text):\n",
        "    return text.strip().split()\n",
        "\n",
        "def regex_tokens(text):\n",
        "    return re.findall(r'\\w+|[^\\w\\s]', text)\n",
        "\n",
        "def char_tokens(text):\n",
        "    return list(text)\n",
        "\n",
        "for text in sentences:\n",
        "    print(f\"\\nğŸ“ Sentence: {text}\")\n",
        "    print(\"â€¢ Whitespace:\", whitespace_tokens(text))\n",
        "    print(\"â€¢ Regex     :\", regex_tokens(text))\n",
        "    print(\"â€¢ Characters:\", char_tokens(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21a6d045",
      "metadata": {
        "id": "21a6d045"
      },
      "source": [
        "## HuggingFace Tokenizers: Model-Level Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "b4aefaa5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4aefaa5",
        "outputId": "25f10c8b-4be0-4840-b7e7-a70210f64ea9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ”§ Model: bert-base-uncased\n",
            "  â€¢ \"I love spicy sandwiches.\" â†’ 5 tokens â†’ ['i', 'love', 'spicy', 'sandwiches', '.']\n",
            "  â€¢ \"Tokenization affects both performance and cost.\" â†’ 8 tokens â†’ ['token', '##ization', 'affects', 'both', 'performance', 'and', 'cost', '.']\n",
            "  â€¢ \"Sheldon likes his sandwiches in isosceles triangles.\" â†’ 10 tokens â†’ ['sheldon', 'likes', 'his', 'sandwiches', 'in', 'iso', '##sc', '##eles', 'triangles', '.']\n",
            "  â€¢ \"ğŸ˜‚ emojis and multilingual à¤¨à¤®à¤¸à¥à¤¤à¥‡ text test subword chops.\" â†’ 19 tokens â†’ ['[UNK]', 'em', '##oj', '##is', 'and', 'multi', '##ling', '##ual', 'à¤¨', '##à¤®', '##à¤¸', '##à¤¤', 'text', 'test', 'sub', '##word', 'chop', '##s', '.']\n",
            "  â€¢ \"Hashtags like #AI and longwords like Donaudampfschifffahrtsgesellschaft are tricky!\" â†’ 25 tokens â†’ ['hash', '##tag', '##s', 'like', '#', 'ai', 'and', 'long', '##words', 'like', 'dona', '##uda', '##mp', '##fs', '##chi', '##ff', '##fa', '##hr', '##ts', '##ges', '##ell', '##schaft', 'are', 'tricky', '!']\n",
            "\n",
            "ğŸ”§ Model: gpt2\n",
            "  â€¢ \"I love spicy sandwiches.\" â†’ 5 tokens â†’ ['I', 'Ä love', 'Ä spicy', 'Ä sandwiches', '.']\n",
            "  â€¢ \"Tokenization affects both performance and cost.\" â†’ 8 tokens â†’ ['Token', 'ization', 'Ä affects', 'Ä both', 'Ä performance', 'Ä and', 'Ä cost', '.']\n",
            "  â€¢ \"Sheldon likes his sandwiches in isosceles triangles.\" â†’ 12 tokens â†’ ['She', 'ldon', 'Ä likes', 'Ä his', 'Ä sandwiches', 'Ä in', 'Ä is', 'os', 'ce', 'les', 'Ä triangles', '.']\n",
            "  â€¢ \"ğŸ˜‚ emojis and multilingual à¤¨à¤®à¤¸à¥à¤¤à¥‡ text test subword chops.\" â†’ 26 tokens â†’ ['Ã°ÅÄº', 'Ä¤', 'Ä em', 'oj', 'is', 'Ä and', 'Ä mult', 'ilingual', 'Ä Ã Â¤', 'Â¨', 'Ã Â¤', 'Â®', 'Ã Â¤', 'Â¸', 'Ã Â¥', 'Ä¯', 'Ã Â¤', 'Â¤', 'Ã Â¥', 'Ä©', 'Ä text', 'Ä test', 'Ä sub', 'word', 'Ä chops', '.']\n",
            "  â€¢ \"Hashtags like #AI and longwords like Donaudampfschifffahrtsgesellschaft are tricky!\" â†’ 26 tokens â†’ ['Hash', 'tags', 'Ä like', 'Ä #', 'AI', 'Ä and', 'Ä long', 'words', 'Ä like', 'Ä Don', 'aud', 'amp', 'fs', 'ch', 'if', 'ff', 'ah', 'r', 'ts', 'ges', 'ells', 'cha', 'ft', 'Ä are', 'Ä tricky', '!']\n",
            "\n",
            "ğŸ”§ Model: roberta-base\n",
            "  â€¢ \"I love spicy sandwiches.\" â†’ 5 tokens â†’ ['I', 'Ä love', 'Ä spicy', 'Ä sandwiches', '.']\n",
            "  â€¢ \"Tokenization affects both performance and cost.\" â†’ 8 tokens â†’ ['Token', 'ization', 'Ä affects', 'Ä both', 'Ä performance', 'Ä and', 'Ä cost', '.']\n",
            "  â€¢ \"Sheldon likes his sandwiches in isosceles triangles.\" â†’ 12 tokens â†’ ['She', 'ldon', 'Ä likes', 'Ä his', 'Ä sandwiches', 'Ä in', 'Ä is', 'os', 'ce', 'les', 'Ä triangles', '.']\n",
            "  â€¢ \"ğŸ˜‚ emojis and multilingual à¤¨à¤®à¤¸à¥à¤¤à¥‡ text test subword chops.\" â†’ 26 tokens â†’ ['Ã°ÅÄº', 'Ä¤', 'Ä em', 'oj', 'is', 'Ä and', 'Ä mult', 'ilingual', 'Ä Ã Â¤', 'Â¨', 'Ã Â¤', 'Â®', 'Ã Â¤', 'Â¸', 'Ã Â¥', 'Ä¯', 'Ã Â¤', 'Â¤', 'Ã Â¥', 'Ä©', 'Ä text', 'Ä test', 'Ä sub', 'word', 'Ä chops', '.']\n",
            "  â€¢ \"Hashtags like #AI and longwords like Donaudampfschifffahrtsgesellschaft are tricky!\" â†’ 26 tokens â†’ ['Hash', 'tags', 'Ä like', 'Ä #', 'AI', 'Ä and', 'Ä long', 'words', 'Ä like', 'Ä Don', 'aud', 'amp', 'fs', 'ch', 'if', 'ff', 'ah', 'r', 'ts', 'ges', 'ells', 'cha', 'ft', 'Ä are', 'Ä tricky', '!']\n",
            "\n",
            "ğŸ”§ Model: xlm-roberta-base\n",
            "  â€¢ \"I love spicy sandwiches.\" â†’ 7 tokens â†’ ['â–I', 'â–love', 'â–spi', 'cy', 'â–sandwich', 'es', '.']\n",
            "  â€¢ \"Tokenization affects both performance and cost.\" â†’ 10 tokens â†’ ['â–To', 'ken', 'ization', 'â–affect', 's', 'â–both', 'â–performance', 'â–and', 'â–cost', '.']\n",
            "  â€¢ \"Sheldon likes his sandwiches in isosceles triangles.\" â†’ 17 tokens â†’ ['â–S', 'held', 'on', 'â–like', 's', 'â–his', 'â–sandwich', 'es', 'â–in', 'â–is', 'os', 'cele', 's', 'â–tri', 'angle', 's', '.']\n",
            "  â€¢ \"ğŸ˜‚ emojis and multilingual à¤¨à¤®à¤¸à¥à¤¤à¥‡ text test subword chops.\" â†’ 17 tokens â†’ ['â–', 'ğŸ˜‚', 'â–emo', 'jis', 'â–and', 'â–multi', 'lingu', 'al', 'â–à¤¨à¤®', 'à¤¸à¥à¤¤à¥‡', 'â–text', 'â–test', 'â–sub', 'word', 'â–cho', 'ps', '.']\n",
            "  â€¢ \"Hashtags like #AI and longwords like Donaudampfschifffahrtsgesellschaft are tricky!\" â†’ 21 tokens â†’ ['â–Hash', 'tags', 'â–like', 'â–#', 'AI', 'â–and', 'â–long', 'word', 's', 'â–like', 'â–Dona', 'uda', 'mpf', 'schiff', 'fahrt', 's', 'gesellschaft', 'â–are', 'â–trick', 'y', '!']\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_ids = [\"bert-base-uncased\", \"gpt2\", \"roberta-base\", \"xlm-roberta-base\"]\n",
        "\n",
        "for model_id in model_ids:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    print(f\"\\nğŸ”§ Model: {model_id}\")\n",
        "    for s in sentences:\n",
        "        tokens = tokenizer.tokenize(s)\n",
        "        print(f\"  â€¢ \\\"{s}\\\" â†’ {len(tokens)} tokens â†’\", tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00361434",
      "metadata": {
        "id": "00361434"
      },
      "source": [
        "## Encoding vs Tokenizing vs Decoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "181135ee",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "181135ee",
        "outputId": "67e7db28-30d3-4af3-a363-548d1c182bd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized : ['Token', 'ization', 'Ä is', 'Ä awesome', '!', 'Ä Ã°Å', 'Ä¼', 'Ä¢']\n",
            "Encoded IDs: [30642, 1634, 318, 7427, 0, 12520, 248, 222]\n",
            "Decoded   : Tokenization is awesome! ğŸš€\n"
          ]
        }
      ],
      "source": [
        "example = \"Tokenization is awesome! ğŸš€\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "print(\"Tokenized :\", tokenizer.tokenize(example))\n",
        "print(\"Encoded IDs:\", tokenizer.encode(example))\n",
        "print(\"Decoded   :\", tokenizer.decode(tokenizer.encode(example)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb982db2",
      "metadata": {
        "id": "fb982db2"
      },
      "source": [
        "## Special Tokens and Model Quirks (BERT Example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "c85d0f38",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c85d0f38",
        "outputId": "20f79e45-e060-4b2f-f3cd-e7767ad7a02f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Special Tokens: {'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}\n",
            "Encoded: [101, 9932, 2003, 4658, 102]\n",
            "Decoded: [CLS] ai is cool [SEP]\n"
          ]
        }
      ],
      "source": [
        "bert_tok = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "print(\"Special Tokens:\", bert_tok.special_tokens_map)\n",
        "ids = bert_tok.encode(\"AI is cool\", add_special_tokens=True)\n",
        "print(\"Encoded:\", ids)\n",
        "print(\"Decoded:\", bert_tok.decode(ids))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c27c2313",
      "metadata": {
        "id": "c27c2313"
      },
      "source": [
        "## Byte-Level Tokenization (GPT-2 Behavior)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "3707cdbc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3707cdbc",
        "outputId": "3facbd64-c03e-4a85-a0cc-73a8312b0dac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Without space: ['hello']\n",
            "With space   : ['Ä hello']\n"
          ]
        }
      ],
      "source": [
        "gpt2_tok = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "print(\"Without space:\", gpt2_tok.tokenize(\"hello\"))\n",
        "print(\"With space   :\", gpt2_tok.tokenize(\" hello\"))  # GPT-2 treats space as a token"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e988714",
      "metadata": {
        "id": "6e988714"
      },
      "source": [
        "## Emoji & Multilingual Tokenization Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "25118090",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25118090",
        "outputId": "51ae00d1-b276-485d-9323-e874a0285e33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input: ğŸ˜‚\n",
            "BERT : ['[UNK]']\n",
            "GPT-2: ['Ã°ÅÄº', 'Ä¤']\n",
            "\n",
            "Input: à¤¨à¤®à¤¸à¥à¤¤à¥‡\n",
            "BERT : ['à¤¨', '##à¤®', '##à¤¸', '##à¤¤']\n",
            "GPT-2: ['Ã Â¤', 'Â¨', 'Ã Â¤', 'Â®', 'Ã Â¤', 'Â¸', 'Ã Â¥', 'Ä¯', 'Ã Â¤', 'Â¤', 'Ã Â¥', 'Ä©']\n",
            "\n",
            "Input: Donaudampfschifffahrtsgesellschaft\n",
            "BERT : ['dona', '##uda', '##mp', '##fs', '##chi', '##ff', '##fa', '##hr', '##ts', '##ges', '##ell', '##schaft']\n",
            "GPT-2: ['Don', 'aud', 'amp', 'fs', 'ch', 'if', 'ff', 'ah', 'r', 'ts', 'ges', 'ells', 'cha', 'ft']\n",
            "\n",
            "Input: #AI\n",
            "BERT : ['#', 'ai']\n",
            "GPT-2: ['#', 'AI']\n",
            "\n",
            "Input: CafÃ©\n",
            "BERT : ['cafe']\n",
            "GPT-2: ['C', 'af', 'ÃƒÂ©']\n"
          ]
        }
      ],
      "source": [
        "samples = [\"ğŸ˜‚\", \"à¤¨à¤®à¤¸à¥à¤¤à¥‡\", \"Donaudampfschifffahrtsgesellschaft\", \"#AI\", \"CafÃ©\"]\n",
        "for s in samples:\n",
        "    print(f\"\\nInput: {s}\")\n",
        "    print(\"BERT :\", AutoTokenizer.from_pretrained(\"bert-base-uncased\").tokenize(s))\n",
        "    print(\"GPT-2:\", AutoTokenizer.from_pretrained(\"gpt2\").tokenize(s))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33d3d608",
      "metadata": {
        "id": "33d3d608"
      },
      "source": [
        "## Vocabulary Peek: What Tokens Are in the Model?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "8f315963",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8f315963",
        "outputId": "13ee09c9-57d2-4724-b523-6875a32c0335"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 20 GPT-2 tokens: [('Ä functional', 10345), ('Ä trunk', 21427), (\"''''\", 39115), ('Ä Indeed', 9676), ('Ä vent', 7435), ('Winged', 47418), ('Ä Boss', 15718), ('favorite', 35200), ('idine', 39422), ('kers', 15949), ('Ä linear', 14174), ('Ä juvenile', 21904), ('ulty', 10672), ('Ä Dee', 29195), ('Ä Swed', 7289), ('Ä LEGO', 29108), ('Tickets', 43254), ('aternal', 14744), ('Ä leaping', 45583), ('Ä mandated', 28853)]\n"
          ]
        }
      ],
      "source": [
        "gpt2_tok = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "vocab = gpt2_tok.get_vocab()\n",
        "top_tokens = list(vocab.items())[:20]\n",
        "print(\"Top 20 GPT-2 tokens:\", top_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40e9c88f",
      "metadata": {
        "id": "40e9c88f"
      },
      "source": [
        "## Padding, Truncation & Sequence Management"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "f8f4b763",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8f4b763",
        "outputId": "d21c236b-3319-4da2-c9f1-328616d82925"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[  101,  2460,  6251,  1012,   102,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0],\n",
            "        [  101,  2023,  2028,  2003,  1037,  2978,  2936,  1998,  3791, 11687,\n",
            "          4667,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
          ]
        }
      ],
      "source": [
        "encoded = bert_tok([\"Short sentence.\", \"This one is a bit longer and needs padding.\"],\n",
        "                   padding=True, truncation=True, return_tensors=\"pt\")\n",
        "print(encoded)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee7b97ef",
      "metadata": {
        "id": "ee7b97ef"
      },
      "source": [
        "## Token Count = Cost\n",
        "Let's estimate how tokens relate to cost (for GPT models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "3203237b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3203237b",
        "outputId": "813eda71-96f6-44fb-f2d7-186a030a698f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens (8): ['Soft', 'Ä k', 'itty', ',', 'Ä warm', 'Ä k', 'itty', '!']\n",
            "Estimated Cost (gpt-4): $ 0.00024\n"
          ]
        }
      ],
      "source": [
        "def estimate_cost(token_count, model=\"gpt-4\"):\n",
        "    prices = {\n",
        "        \"gpt-3.5\": 0.0015 / 1000,\n",
        "        \"gpt-4\": 0.03 / 1000,\n",
        "        \"gpt-4o\": 0.005 / 1000\n",
        "    }\n",
        "    return round(token_count * prices.get(model, 0), 6)\n",
        "\n",
        "text = \"Soft kitty, warm kitty!\"\n",
        "tok = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "tokens = tok.tokenize(text)\n",
        "print(f\"Tokens ({len(tokens)}): {tokens}\")\n",
        "print(\"Estimated Cost (gpt-4): $\", estimate_cost(len(tokens), model=\"gpt-4\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b4c82ef",
      "metadata": {
        "id": "6b4c82ef"
      },
      "source": [
        "## Wrap-Up: Tokenization = Prep Chef of LLMs\n",
        "- Before embeddings, before transformers, before predictions we slice.\n",
        "- The type of slicing affects cost, performance, and accuracy.\n",
        "- Choose the right tokenizer for the job and never underestimate the power of prep work."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}