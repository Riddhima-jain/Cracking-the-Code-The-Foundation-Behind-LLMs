{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "84ec3ee5",
      "metadata": {
        "id": "84ec3ee5"
      },
      "source": [
        "# Tokenization Deep Dive: How LLMs Slice Text\n",
        "**Before an LLM tastes your words, it needs to slice them.**\n",
        "\n",
        "In our companion post, we likened tokenization to slicing a sandwich. Now, let’s go deeper.\n",
        "\n",
        "From whitespace to byte-level slicing and explore how different models tokenize inputs, decode them back, and how this all ties into cost and performance.\n",
        "\n",
        "_Let’s begin at the café counter..._"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64098fd6",
      "metadata": {
        "id": "64098fd6"
      },
      "source": [
        "## Analogy Recap: Your Sentence = A Sandwich\n",
        "Each token is a slice. Different slicing tools lead to different results:\n",
        "- **Whitespace Tokenization** → Clean visible cuts\n",
        "- **Subword Tokenization** → Balanced bites like 'Sand' + '##wich'\n",
        "- **Character Tokenization** → Micro-crumbs\n",
        "- **Advanced Models** use: BPE, WordPiece, SentencePiece\n",
        "\n",
        "> More slices = more tokens = more cost."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "266cad1d",
      "metadata": {
        "id": "266cad1d"
      },
      "source": [
        "## Let's Start with a Few Sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "115bfa7a",
      "metadata": {
        "id": "115bfa7a"
      },
      "outputs": [],
      "source": [
        "sentences = [\n",
        "    \"I love spicy sandwiches.\",\n",
        "    \"Tokenization affects both performance and cost.\",\n",
        "    \"Sheldon likes his sandwiches in isosceles triangles.\",\n",
        "    \"😂 emojis and multilingual नमस्ते text test subword chops.\",\n",
        "    \"Hashtags like #AI and longwords like Donaudampfschifffahrtsgesellschaft are tricky!\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e498d61",
      "metadata": {
        "id": "6e498d61"
      },
      "source": [
        "## Custom Tokenizers: Whitespace, Regex, Character"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "e713caea",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e713caea",
        "outputId": "2ebda799-cf09-46a8-f8cc-152ef229f034"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📝 Sentence: I love spicy sandwiches.\n",
            "• Whitespace: ['I', 'love', 'spicy', 'sandwiches.']\n",
            "• Regex     : ['I', 'love', 'spicy', 'sandwiches', '.']\n",
            "• Characters: ['I', ' ', 'l', 'o', 'v', 'e', ' ', 's', 'p', 'i', 'c', 'y', ' ', 's', 'a', 'n', 'd', 'w', 'i', 'c', 'h', 'e', 's', '.']\n",
            "\n",
            "📝 Sentence: Tokenization affects both performance and cost.\n",
            "• Whitespace: ['Tokenization', 'affects', 'both', 'performance', 'and', 'cost.']\n",
            "• Regex     : ['Tokenization', 'affects', 'both', 'performance', 'and', 'cost', '.']\n",
            "• Characters: ['T', 'o', 'k', 'e', 'n', 'i', 'z', 'a', 't', 'i', 'o', 'n', ' ', 'a', 'f', 'f', 'e', 'c', 't', 's', ' ', 'b', 'o', 't', 'h', ' ', 'p', 'e', 'r', 'f', 'o', 'r', 'm', 'a', 'n', 'c', 'e', ' ', 'a', 'n', 'd', ' ', 'c', 'o', 's', 't', '.']\n",
            "\n",
            "📝 Sentence: Sheldon likes his sandwiches in isosceles triangles.\n",
            "• Whitespace: ['Sheldon', 'likes', 'his', 'sandwiches', 'in', 'isosceles', 'triangles.']\n",
            "• Regex     : ['Sheldon', 'likes', 'his', 'sandwiches', 'in', 'isosceles', 'triangles', '.']\n",
            "• Characters: ['S', 'h', 'e', 'l', 'd', 'o', 'n', ' ', 'l', 'i', 'k', 'e', 's', ' ', 'h', 'i', 's', ' ', 's', 'a', 'n', 'd', 'w', 'i', 'c', 'h', 'e', 's', ' ', 'i', 'n', ' ', 'i', 's', 'o', 's', 'c', 'e', 'l', 'e', 's', ' ', 't', 'r', 'i', 'a', 'n', 'g', 'l', 'e', 's', '.']\n",
            "\n",
            "📝 Sentence: 😂 emojis and multilingual नमस्ते text test subword chops.\n",
            "• Whitespace: ['😂', 'emojis', 'and', 'multilingual', 'नमस्ते', 'text', 'test', 'subword', 'chops.']\n",
            "• Regex     : ['😂', 'emojis', 'and', 'multilingual', 'नमस', '्', 'त', 'े', 'text', 'test', 'subword', 'chops', '.']\n",
            "• Characters: ['😂', ' ', 'e', 'm', 'o', 'j', 'i', 's', ' ', 'a', 'n', 'd', ' ', 'm', 'u', 'l', 't', 'i', 'l', 'i', 'n', 'g', 'u', 'a', 'l', ' ', 'न', 'म', 'स', '्', 'त', 'े', ' ', 't', 'e', 'x', 't', ' ', 't', 'e', 's', 't', ' ', 's', 'u', 'b', 'w', 'o', 'r', 'd', ' ', 'c', 'h', 'o', 'p', 's', '.']\n",
            "\n",
            "📝 Sentence: Hashtags like #AI and longwords like Donaudampfschifffahrtsgesellschaft are tricky!\n",
            "• Whitespace: ['Hashtags', 'like', '#AI', 'and', 'longwords', 'like', 'Donaudampfschifffahrtsgesellschaft', 'are', 'tricky!']\n",
            "• Regex     : ['Hashtags', 'like', '#', 'AI', 'and', 'longwords', 'like', 'Donaudampfschifffahrtsgesellschaft', 'are', 'tricky', '!']\n",
            "• Characters: ['H', 'a', 's', 'h', 't', 'a', 'g', 's', ' ', 'l', 'i', 'k', 'e', ' ', '#', 'A', 'I', ' ', 'a', 'n', 'd', ' ', 'l', 'o', 'n', 'g', 'w', 'o', 'r', 'd', 's', ' ', 'l', 'i', 'k', 'e', ' ', 'D', 'o', 'n', 'a', 'u', 'd', 'a', 'm', 'p', 'f', 's', 'c', 'h', 'i', 'f', 'f', 'f', 'a', 'h', 'r', 't', 's', 'g', 'e', 's', 'e', 'l', 'l', 's', 'c', 'h', 'a', 'f', 't', ' ', 'a', 'r', 'e', ' ', 't', 'r', 'i', 'c', 'k', 'y', '!']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def whitespace_tokens(text):\n",
        "    return text.strip().split()\n",
        "\n",
        "def regex_tokens(text):\n",
        "    return re.findall(r'\\w+|[^\\w\\s]', text)\n",
        "\n",
        "def char_tokens(text):\n",
        "    return list(text)\n",
        "\n",
        "for text in sentences:\n",
        "    print(f\"\\n📝 Sentence: {text}\")\n",
        "    print(\"• Whitespace:\", whitespace_tokens(text))\n",
        "    print(\"• Regex     :\", regex_tokens(text))\n",
        "    print(\"• Characters:\", char_tokens(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21a6d045",
      "metadata": {
        "id": "21a6d045"
      },
      "source": [
        "## HuggingFace Tokenizers: Model-Level Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "b4aefaa5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4aefaa5",
        "outputId": "25f10c8b-4be0-4840-b7e7-a70210f64ea9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔧 Model: bert-base-uncased\n",
            "  • \"I love spicy sandwiches.\" → 5 tokens → ['i', 'love', 'spicy', 'sandwiches', '.']\n",
            "  • \"Tokenization affects both performance and cost.\" → 8 tokens → ['token', '##ization', 'affects', 'both', 'performance', 'and', 'cost', '.']\n",
            "  • \"Sheldon likes his sandwiches in isosceles triangles.\" → 10 tokens → ['sheldon', 'likes', 'his', 'sandwiches', 'in', 'iso', '##sc', '##eles', 'triangles', '.']\n",
            "  • \"😂 emojis and multilingual नमस्ते text test subword chops.\" → 19 tokens → ['[UNK]', 'em', '##oj', '##is', 'and', 'multi', '##ling', '##ual', 'न', '##म', '##स', '##त', 'text', 'test', 'sub', '##word', 'chop', '##s', '.']\n",
            "  • \"Hashtags like #AI and longwords like Donaudampfschifffahrtsgesellschaft are tricky!\" → 25 tokens → ['hash', '##tag', '##s', 'like', '#', 'ai', 'and', 'long', '##words', 'like', 'dona', '##uda', '##mp', '##fs', '##chi', '##ff', '##fa', '##hr', '##ts', '##ges', '##ell', '##schaft', 'are', 'tricky', '!']\n",
            "\n",
            "🔧 Model: gpt2\n",
            "  • \"I love spicy sandwiches.\" → 5 tokens → ['I', 'Ġlove', 'Ġspicy', 'Ġsandwiches', '.']\n",
            "  • \"Tokenization affects both performance and cost.\" → 8 tokens → ['Token', 'ization', 'Ġaffects', 'Ġboth', 'Ġperformance', 'Ġand', 'Ġcost', '.']\n",
            "  • \"Sheldon likes his sandwiches in isosceles triangles.\" → 12 tokens → ['She', 'ldon', 'Ġlikes', 'Ġhis', 'Ġsandwiches', 'Ġin', 'Ġis', 'os', 'ce', 'les', 'Ġtriangles', '.']\n",
            "  • \"😂 emojis and multilingual नमस्ते text test subword chops.\" → 26 tokens → ['ðŁĺ', 'Ĥ', 'Ġem', 'oj', 'is', 'Ġand', 'Ġmult', 'ilingual', 'Ġà¤', '¨', 'à¤', '®', 'à¤', '¸', 'à¥', 'į', 'à¤', '¤', 'à¥', 'ĩ', 'Ġtext', 'Ġtest', 'Ġsub', 'word', 'Ġchops', '.']\n",
            "  • \"Hashtags like #AI and longwords like Donaudampfschifffahrtsgesellschaft are tricky!\" → 26 tokens → ['Hash', 'tags', 'Ġlike', 'Ġ#', 'AI', 'Ġand', 'Ġlong', 'words', 'Ġlike', 'ĠDon', 'aud', 'amp', 'fs', 'ch', 'if', 'ff', 'ah', 'r', 'ts', 'ges', 'ells', 'cha', 'ft', 'Ġare', 'Ġtricky', '!']\n",
            "\n",
            "🔧 Model: roberta-base\n",
            "  • \"I love spicy sandwiches.\" → 5 tokens → ['I', 'Ġlove', 'Ġspicy', 'Ġsandwiches', '.']\n",
            "  • \"Tokenization affects both performance and cost.\" → 8 tokens → ['Token', 'ization', 'Ġaffects', 'Ġboth', 'Ġperformance', 'Ġand', 'Ġcost', '.']\n",
            "  • \"Sheldon likes his sandwiches in isosceles triangles.\" → 12 tokens → ['She', 'ldon', 'Ġlikes', 'Ġhis', 'Ġsandwiches', 'Ġin', 'Ġis', 'os', 'ce', 'les', 'Ġtriangles', '.']\n",
            "  • \"😂 emojis and multilingual नमस्ते text test subword chops.\" → 26 tokens → ['ðŁĺ', 'Ĥ', 'Ġem', 'oj', 'is', 'Ġand', 'Ġmult', 'ilingual', 'Ġà¤', '¨', 'à¤', '®', 'à¤', '¸', 'à¥', 'į', 'à¤', '¤', 'à¥', 'ĩ', 'Ġtext', 'Ġtest', 'Ġsub', 'word', 'Ġchops', '.']\n",
            "  • \"Hashtags like #AI and longwords like Donaudampfschifffahrtsgesellschaft are tricky!\" → 26 tokens → ['Hash', 'tags', 'Ġlike', 'Ġ#', 'AI', 'Ġand', 'Ġlong', 'words', 'Ġlike', 'ĠDon', 'aud', 'amp', 'fs', 'ch', 'if', 'ff', 'ah', 'r', 'ts', 'ges', 'ells', 'cha', 'ft', 'Ġare', 'Ġtricky', '!']\n",
            "\n",
            "🔧 Model: xlm-roberta-base\n",
            "  • \"I love spicy sandwiches.\" → 7 tokens → ['▁I', '▁love', '▁spi', 'cy', '▁sandwich', 'es', '.']\n",
            "  • \"Tokenization affects both performance and cost.\" → 10 tokens → ['▁To', 'ken', 'ization', '▁affect', 's', '▁both', '▁performance', '▁and', '▁cost', '.']\n",
            "  • \"Sheldon likes his sandwiches in isosceles triangles.\" → 17 tokens → ['▁S', 'held', 'on', '▁like', 's', '▁his', '▁sandwich', 'es', '▁in', '▁is', 'os', 'cele', 's', '▁tri', 'angle', 's', '.']\n",
            "  • \"😂 emojis and multilingual नमस्ते text test subword chops.\" → 17 tokens → ['▁', '😂', '▁emo', 'jis', '▁and', '▁multi', 'lingu', 'al', '▁नम', 'स्ते', '▁text', '▁test', '▁sub', 'word', '▁cho', 'ps', '.']\n",
            "  • \"Hashtags like #AI and longwords like Donaudampfschifffahrtsgesellschaft are tricky!\" → 21 tokens → ['▁Hash', 'tags', '▁like', '▁#', 'AI', '▁and', '▁long', 'word', 's', '▁like', '▁Dona', 'uda', 'mpf', 'schiff', 'fahrt', 's', 'gesellschaft', '▁are', '▁trick', 'y', '!']\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_ids = [\"bert-base-uncased\", \"gpt2\", \"roberta-base\", \"xlm-roberta-base\"]\n",
        "\n",
        "for model_id in model_ids:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    print(f\"\\n🔧 Model: {model_id}\")\n",
        "    for s in sentences:\n",
        "        tokens = tokenizer.tokenize(s)\n",
        "        print(f\"  • \\\"{s}\\\" → {len(tokens)} tokens →\", tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00361434",
      "metadata": {
        "id": "00361434"
      },
      "source": [
        "## Encoding vs Tokenizing vs Decoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "181135ee",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "181135ee",
        "outputId": "67e7db28-30d3-4af3-a363-548d1c182bd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized : ['Token', 'ization', 'Ġis', 'Ġawesome', '!', 'ĠðŁ', 'ļ', 'Ģ']\n",
            "Encoded IDs: [30642, 1634, 318, 7427, 0, 12520, 248, 222]\n",
            "Decoded   : Tokenization is awesome! 🚀\n"
          ]
        }
      ],
      "source": [
        "example = \"Tokenization is awesome! 🚀\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "print(\"Tokenized :\", tokenizer.tokenize(example))\n",
        "print(\"Encoded IDs:\", tokenizer.encode(example))\n",
        "print(\"Decoded   :\", tokenizer.decode(tokenizer.encode(example)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb982db2",
      "metadata": {
        "id": "fb982db2"
      },
      "source": [
        "## Special Tokens and Model Quirks (BERT Example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "c85d0f38",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c85d0f38",
        "outputId": "20f79e45-e060-4b2f-f3cd-e7767ad7a02f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Special Tokens: {'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}\n",
            "Encoded: [101, 9932, 2003, 4658, 102]\n",
            "Decoded: [CLS] ai is cool [SEP]\n"
          ]
        }
      ],
      "source": [
        "bert_tok = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "print(\"Special Tokens:\", bert_tok.special_tokens_map)\n",
        "ids = bert_tok.encode(\"AI is cool\", add_special_tokens=True)\n",
        "print(\"Encoded:\", ids)\n",
        "print(\"Decoded:\", bert_tok.decode(ids))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c27c2313",
      "metadata": {
        "id": "c27c2313"
      },
      "source": [
        "## Byte-Level Tokenization (GPT-2 Behavior)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "3707cdbc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3707cdbc",
        "outputId": "3facbd64-c03e-4a85-a0cc-73a8312b0dac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Without space: ['hello']\n",
            "With space   : ['Ġhello']\n"
          ]
        }
      ],
      "source": [
        "gpt2_tok = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "print(\"Without space:\", gpt2_tok.tokenize(\"hello\"))\n",
        "print(\"With space   :\", gpt2_tok.tokenize(\" hello\"))  # GPT-2 treats space as a token"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e988714",
      "metadata": {
        "id": "6e988714"
      },
      "source": [
        "## Emoji & Multilingual Tokenization Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "25118090",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25118090",
        "outputId": "51ae00d1-b276-485d-9323-e874a0285e33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input: 😂\n",
            "BERT : ['[UNK]']\n",
            "GPT-2: ['ðŁĺ', 'Ĥ']\n",
            "\n",
            "Input: नमस्ते\n",
            "BERT : ['न', '##म', '##स', '##त']\n",
            "GPT-2: ['à¤', '¨', 'à¤', '®', 'à¤', '¸', 'à¥', 'į', 'à¤', '¤', 'à¥', 'ĩ']\n",
            "\n",
            "Input: Donaudampfschifffahrtsgesellschaft\n",
            "BERT : ['dona', '##uda', '##mp', '##fs', '##chi', '##ff', '##fa', '##hr', '##ts', '##ges', '##ell', '##schaft']\n",
            "GPT-2: ['Don', 'aud', 'amp', 'fs', 'ch', 'if', 'ff', 'ah', 'r', 'ts', 'ges', 'ells', 'cha', 'ft']\n",
            "\n",
            "Input: #AI\n",
            "BERT : ['#', 'ai']\n",
            "GPT-2: ['#', 'AI']\n",
            "\n",
            "Input: Café\n",
            "BERT : ['cafe']\n",
            "GPT-2: ['C', 'af', 'Ã©']\n"
          ]
        }
      ],
      "source": [
        "samples = [\"😂\", \"नमस्ते\", \"Donaudampfschifffahrtsgesellschaft\", \"#AI\", \"Café\"]\n",
        "for s in samples:\n",
        "    print(f\"\\nInput: {s}\")\n",
        "    print(\"BERT :\", AutoTokenizer.from_pretrained(\"bert-base-uncased\").tokenize(s))\n",
        "    print(\"GPT-2:\", AutoTokenizer.from_pretrained(\"gpt2\").tokenize(s))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33d3d608",
      "metadata": {
        "id": "33d3d608"
      },
      "source": [
        "## Vocabulary Peek: What Tokens Are in the Model?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "8f315963",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8f315963",
        "outputId": "13ee09c9-57d2-4724-b523-6875a32c0335"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 20 GPT-2 tokens: [('Ġfunctional', 10345), ('Ġtrunk', 21427), (\"''''\", 39115), ('ĠIndeed', 9676), ('Ġvent', 7435), ('Winged', 47418), ('ĠBoss', 15718), ('favorite', 35200), ('idine', 39422), ('kers', 15949), ('Ġlinear', 14174), ('Ġjuvenile', 21904), ('ulty', 10672), ('ĠDee', 29195), ('ĠSwed', 7289), ('ĠLEGO', 29108), ('Tickets', 43254), ('aternal', 14744), ('Ġleaping', 45583), ('Ġmandated', 28853)]\n"
          ]
        }
      ],
      "source": [
        "gpt2_tok = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "vocab = gpt2_tok.get_vocab()\n",
        "top_tokens = list(vocab.items())[:20]\n",
        "print(\"Top 20 GPT-2 tokens:\", top_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40e9c88f",
      "metadata": {
        "id": "40e9c88f"
      },
      "source": [
        "## Padding, Truncation & Sequence Management"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "f8f4b763",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8f4b763",
        "outputId": "d21c236b-3319-4da2-c9f1-328616d82925"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[  101,  2460,  6251,  1012,   102,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0],\n",
            "        [  101,  2023,  2028,  2003,  1037,  2978,  2936,  1998,  3791, 11687,\n",
            "          4667,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
          ]
        }
      ],
      "source": [
        "encoded = bert_tok([\"Short sentence.\", \"This one is a bit longer and needs padding.\"],\n",
        "                   padding=True, truncation=True, return_tensors=\"pt\")\n",
        "print(encoded)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee7b97ef",
      "metadata": {
        "id": "ee7b97ef"
      },
      "source": [
        "## Token Count = Cost\n",
        "Let's estimate how tokens relate to cost (for GPT models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "3203237b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3203237b",
        "outputId": "813eda71-96f6-44fb-f2d7-186a030a698f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens (8): ['Soft', 'Ġk', 'itty', ',', 'Ġwarm', 'Ġk', 'itty', '!']\n",
            "Estimated Cost (gpt-4): $ 0.00024\n"
          ]
        }
      ],
      "source": [
        "def estimate_cost(token_count, model=\"gpt-4\"):\n",
        "    prices = {\n",
        "        \"gpt-3.5\": 0.0015 / 1000,\n",
        "        \"gpt-4\": 0.03 / 1000,\n",
        "        \"gpt-4o\": 0.005 / 1000\n",
        "    }\n",
        "    return round(token_count * prices.get(model, 0), 6)\n",
        "\n",
        "text = \"Soft kitty, warm kitty!\"\n",
        "tok = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "tokens = tok.tokenize(text)\n",
        "print(f\"Tokens ({len(tokens)}): {tokens}\")\n",
        "print(\"Estimated Cost (gpt-4): $\", estimate_cost(len(tokens), model=\"gpt-4\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b4c82ef",
      "metadata": {
        "id": "6b4c82ef"
      },
      "source": [
        "## Wrap-Up: Tokenization = Prep Chef of LLMs\n",
        "- Before embeddings, before transformers, before predictions we slice.\n",
        "- The type of slicing affects cost, performance, and accuracy.\n",
        "- Choose the right tokenizer for the job and never underestimate the power of prep work."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}