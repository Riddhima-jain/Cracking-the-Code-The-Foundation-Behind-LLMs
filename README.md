# ðŸ“˜ Cracking the Code: The Foundation Behind LLMs

Welcome to the companion repository for the Medium series **_Cracking the Code: The Foundational Math Behind LLMs_** â€” a storytelling-driven journey into the building blocks of large language models like GPT, Claude, and Gemini.

Whether you're a developer, a curious learner, or an ML enthusiast trying to go beyond surface-level understanding, this series blends **math, metaphor, and motion** to help you connect the dots.

---

## ðŸ§  About the Series

Each episode explains core concepts behind LLMs using:
- ðŸµ **Flavourful analogies** (chai vs. coffee, dance floor spotlightsâ€¦)
- ðŸŽ¨ **Intuitive visuals & animations**
- ðŸ§ª **Executable Jupyter notebooks**
- ðŸ¤– **Pretrained model demos** (GloVe, Word2Vec, HuggingFace)

You wonâ€™t just learn *what* the model is doing â€” you'll understand *why* it works.

---

## ðŸ“š Episode List

### âœ… Episode 1 â€“ [How Computers 'Taste' Words: Word Embeddings Explained](https://medium.com/@jainriddhima00/how-computers-taste-words-word-embeddings-explained-06b6d5ab15a6)
> Notebook: [`Word_Embeddings.ipynb`](https://github.com/Riddhima-jain/Cracking-the-Code-The-Foundation-Behind-LLMs/blob/main/Word_Embeddings.ipynb)

What youâ€™ll explore:
- What are embeddings & why text needs them
- Vector spaces, cosine similarity, and flavor metaphors
- Word2Vec (CBOW & Skip-Gram)
- Analogy math: `king - man + woman = queen`
- PCA visualization of real GloVe embeddings

---

### âœ… Episode 2 â€“ [How AI Slices Language: Tokenization Deep Dive](https://www.linkedin.com/posts/jainriddhima00_tokenization-llm-nlp-activity-7204826815171104769-rKkS/)
> Notebook: [`Tokenization_Deep_Dive.ipynb`](https://github.com/Riddhima-jain/Cracking-the-Code-The-Foundation-Behind-LLMs/blob/main/Tokenization_Deep_Dive.ipynb)

Whatâ€™s inside:
- Why tokenization matters (and how it affects cost!)
- Methods: Whitespace, BPE, WordPiece, SentencePiece, Character
- Real slicing demos with API-style cost comparison
- Sandwich metaphors & tokenizer breakdowns

---

### âœ… Episode 3 â€“ [How AI Pays Attention: Spotlights, Sequence & Focus](https://medium.com/@jainriddhima00/how-ai-pays-attention-spotlights-context-and-positional-cues-e5e0b8ea15e7)
> Notebook: [`Tokenization_Deep_Dive.ipynb`](https://github.com/Riddhima-jain/Cracking-the-Code-The-Foundation-Behind-LLMs/blob/main/PositionalEncodings_And_Attention.ipynb)


Learn how:
- Positional encoding adds order to parallel tokens
- Self-attention decides â€œwho to watchâ€
- Multi-head attention brings diverse focus  
Includes spotlight dance floor metaphors and a visual walk-through of attention flow.

---

### âœ… Episode 4 â€“ [From Gossip to Insight: Feedforward Networks & Activation Functions](https://medium.com/@jainriddhima00) *(https://medium.com/@jainriddhima00/from-gossip-to-insight-understanding-feedforward-networks-activation-functions-f626c8115e08?sk=cb1bb40602d5aa8ad007e03f9150e1ce)*  
> Notebook: [`FFN_Activations.ipynb`](https://github.com/Riddhima-jain/Cracking-the-Code-The-Foundation-Behind-LLMs/blob/main/FFN_Activations.ipynb)  
> Interactive Demos: [FFN Demo](https://riddhima-jain.github.io/Cracking-the-Code-The-Foundation-Behind-LLMs/demos/ffn_demo.min.html) | [Activations Demo](https://riddhima-jain.github.io/Cracking-the-Code-The-Foundation-Behind-LLMs/demos/activations_demo.min.html)

Whatâ€™s covered:
- Feedforward Networks (FFNs) as the **gossip table** where tokens get refined
- Breaking down **weights, biases, and activations**
- The â€œexpand â†’ transform â†’ compressâ€ pipeline
- Activation personalities (ReLU, Sigmoid, Tanh, GELU, Swish)
- Interactive visualizations: how stories (signals) get reshaped  
*(inspired by NN-SVG by Alex Lenail for diagrams)*

---

## ðŸ”œ Coming Soon

| Episode | Theme |
|--------|-------|
| Episode 5 | **How AI Learns From Mistakes â€” Loss Functions & Gradient Descent** |
| Episode 6 | **Attention is All You Need â€” The Transformer Revolution** |

---

---

## ðŸ§ª How to Use This Repo

1. Clone the repo or open notebooks in Colab.
2. Each episodeâ€™s notebook is plug-and-play.
3. Read the accompanying Medium article for full context and visuals.

---

## ðŸ› ï¸ Contributing

Spotted a bug?  
Have a better analogy?  
Want to contribute code or visuals?

Feel free to open an issue or pull request.  
Letâ€™s build accessible AI education together.

---

## â­ï¸ Found this Helpful?

- Give the repo a star ðŸŒŸ  
- Share the [Medium series](https://medium.com/@jainriddhima00)  
- Follow for more episodes, visuals, and explainers!

> â€œItâ€™s not the tech. Itâ€™s the story you tell with it.â€  
> â€” *Iron Man*

---

ðŸ§µ Letâ€™s decode the magic â€” one sip, one spotlight, one dance move at a time.
