# 📘 Cracking the Code: The Foundation Behind LLMs

Welcome to the companion repository for the Medium series **_Cracking the Code: The Foundational Math Behind LLMs_** — a storytelling-driven journey into the building blocks of large language models like GPT, Claude, and Gemini.

Whether you're a developer, a curious learner, or an ML enthusiast trying to go beyond surface-level understanding, this series blends **math, metaphor, and motion** to help you connect the dots.

---

## 🧠 About the Series

Each episode explains core concepts behind LLMs using:
- 🍵 **Flavourful analogies** (chai vs. coffee, dance floor spotlights…)
- 🎨 **Intuitive visuals & animations**
- 🧪 **Executable Jupyter notebooks**
- 🤖 **Pretrained model demos** (GloVe, Word2Vec, HuggingFace)

You won’t just learn *what* the model is doing — you'll understand *why* it works.

---

## 📚 Episode List

### ✅ Episode 1 – [How Computers 'Taste' Words: Word Embeddings Explained](https://medium.com/@jainriddhima00/how-computers-taste-words-word-embeddings-explained-06b6d5ab15a6)
> Notebook: [`Word_Embeddings.ipynb`](https://github.com/Riddhima-jain/Cracking-the-Code-The-Foundation-Behind-LLMs/blob/main/Word_Embeddings.ipynb)

What you’ll explore:
- What are embeddings & why text needs them
- Vector spaces, cosine similarity, and flavor metaphors
- Word2Vec (CBOW & Skip-Gram)
- Analogy math: `king - man + woman = queen`
- PCA visualization of real GloVe embeddings

---

### ✅ Episode 2 – [How AI Slices Language: Tokenization Deep Dive](https://www.linkedin.com/posts/jainriddhima00_tokenization-llm-nlp-activity-7204826815171104769-rKkS/)
> Notebook: [`Tokenization_Deep_Dive.ipynb`](https://github.com/Riddhima-jain/Cracking-the-Code-The-Foundation-Behind-LLMs/blob/main/Tokenization_Deep_Dive.ipynb)

What’s inside:
- Why tokenization matters (and how it affects cost!)
- Methods: Whitespace, BPE, WordPiece, SentencePiece, Character
- Real slicing demos with API-style cost comparison
- Sandwich metaphors & tokenizer breakdowns

---

### ✅ Episode 3 – [How AI Pays Attention: Spotlights, Sequence & Focus](https://medium.com/@jainriddhima00/how-ai-pays-attention-spotlights-context-and-positional-cues-e5e0b8ea15e7)
> Notebook: [`Tokenization_Deep_Dive.ipynb`](https://github.com/Riddhima-jain/Cracking-the-Code-The-Foundation-Behind-LLMs/blob/main/PositionalEncodings_And_Attention.ipynb)


Learn how:
- Positional encoding adds order to parallel tokens
- Self-attention decides “who to watch”
- Multi-head attention brings diverse focus  
Includes spotlight dance floor metaphors and a visual walk-through of attention flow.

---

### ✅ Episode 4 – [From Gossip to Insight: Feedforward Networks & Activation Functions](https://medium.com/@jainriddhima00) *(https://medium.com/@jainriddhima00/from-gossip-to-insight-understanding-feedforward-networks-activation-functions-f626c8115e08?sk=cb1bb40602d5aa8ad007e03f9150e1ce)*  
> Notebook: [`FFN_Activations.ipynb`](https://github.com/Riddhima-jain/Cracking-the-Code-The-Foundation-Behind-LLMs/blob/main/FFN_Activations.ipynb)  
> Interactive Demos: [FFN Demo](https://riddhima-jain.github.io/Cracking-the-Code-The-Foundation-Behind-LLMs/demos/ffn_demo.min.html) | [Activations Demo](https://riddhima-jain.github.io/Cracking-the-Code-The-Foundation-Behind-LLMs/demos/activations_demo.min.html)

What’s covered:
- Feedforward Networks (FFNs) as the **gossip table** where tokens get refined
- Breaking down **weights, biases, and activations**
- The “expand → transform → compress” pipeline
- Activation personalities (ReLU, Sigmoid, Tanh, GELU, Swish)
- Interactive visualizations: how stories (signals) get reshaped  
*(inspired by NN-SVG by Alex Lenail for diagrams)*

---

## 🔜 Coming Soon

| Episode | Theme |
|--------|-------|
| Episode 5 | **How AI Learns From Mistakes — Loss Functions & Gradient Descent** |
| Episode 6 | **Attention is All You Need — The Transformer Revolution** |

---

---

## 🧪 How to Use This Repo

1. Clone the repo or open notebooks in Colab.
2. Each episode’s notebook is plug-and-play.
3. Read the accompanying Medium article for full context and visuals.

---

## 🛠️ Contributing

Spotted a bug?  
Have a better analogy?  
Want to contribute code or visuals?

Feel free to open an issue or pull request.  
Let’s build accessible AI education together.

---

## ⭐️ Found this Helpful?

- Give the repo a star 🌟  
- Share the [Medium series](https://medium.com/@jainriddhima00)  
- Follow for more episodes, visuals, and explainers!

> “It’s not the tech. It’s the story you tell with it.”  
> — *Iron Man*

---

🧵 Let’s decode the magic — one sip, one spotlight, one dance move at a time.
