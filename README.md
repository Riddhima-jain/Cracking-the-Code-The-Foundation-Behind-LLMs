# Cracking-the-Code-The-Foundation-Behind-LLMs

Welcome to the companion repository for the Medium series _Cracking the Code: The Foundational Math Behind LLMs_ — a storytelling-driven journey through the core mathematical ideas that power large language models like GPT, Gemini, Claude through easy to remember concepts.

## About the Series

This series simplifies complex AI math using **flavourful analogies**, **visualizations**, and **hands-on code**. Whether you're a curious developer, an early-career ML enthusiast, or someone transitioning into AI, this series is for you.

Each episode comes with:
- Intuitive analogies (chai vs. coffee... yes, really!)
- Visual illustrations & plots
- Fully executable notebooks
- Real pre-trained model demos (GloVe/Word2Vec)

---

## Episode List

### **Episode 1 – How Computers 'Taste' Words: Word Embeddings Explained**
> Medium article: [Link to Medium post]()  
> Notebook: [`word_embeddings_episode_1.ipynb`]()

What you'll learn:
- What are word embeddings?
- Why AI models don't work with plain text
- Word2Vec: CBOW vs. Skip-Gram architectures
- Cosine similarity (explained with vector math & beverages ☕️)
- Word analogies like `king - man + woman = queen`
- PCA visualization of real GloVe embeddings

---

## How to Use This Repo

> Follow the instructions in the notebook and the articles!

## Coming Soon
> How AI Learns from Mistakes – Loss Functions & Gradient Descent
> Attention is All You Need – The Magic Behind Transformers	

## Contributing
Spotted a bug? Have a better analogy? Want to contribute a notebook for a future episode?
Feel free to open an issue or reach out to me!

## Found this helpful?
Please star this repo and share the Medium article to support thoughtful, accessible AI content.

"It's not the tech. It's the story you tell with it."
— Iron Man
