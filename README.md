# Cracking-the-Code-The-Foundation-Behind-LLMs

Welcome to the companion repository for the Medium series _Cracking the Code: The Foundational Math Behind LLMs_ â€” a storytelling-driven journey through the core mathematical ideas that power large language models like GPT, Gemini, Claude through easy to remember concepts.

## About the Series

This series simplifies complex AI math using **flavourful analogies**, **visualizations**, and **hands-on code**. Whether you're a curious developer, an early-career ML enthusiast, or someone transitioning into AI, this series is for you.

Each episode comes with:
- Intuitive analogies (chai vs. coffee... yes, really!)
- Visual illustrations & plots
- Fully executable notebooks
- Real pre-trained model demos (GloVe/Word2Vec)

---

## ğŸ”¥ Episode List

### **Episode 1 â€“ How Computers 'Taste' Words: Word Embeddings Explained**
> Medium article: [Link to Medium post]()  
> Notebook: [`word_embeddings_episode_1.ipynb`]()

What you'll learn:
- What are word embeddings?
- Why AI models don't work with plain text
- Word2Vec: CBOW vs. Skip-Gram architectures
- Cosine similarity (explained with vector math & beverages â˜•ï¸)
- Word analogies like `king - man + woman = queen`
- PCA visualization of real GloVe embeddings

---

## How to Use This Repo

> Follow the instructions in the notebook and the articles!

Coming Soon
Episode	Title	Status
2	How AI Learns from Mistakes â€“ Loss Functions & Gradient Descent (ğŸ•º Dance Analogy!)	ğŸ› ï¸ In Progress
3	Attention is All You Need â€“ The Magic Behind Transformers	ğŸ› ï¸ Coming Soon
4	Positional Encoding â€“ How Models Understand Word Order	ğŸ”œ Queued

Contributing
Spotted a bug? Have a better analogy? Want to contribute a notebook for a future episode?
Feel free to open an issue or reach out to me!
